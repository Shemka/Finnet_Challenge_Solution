{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "solution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3987xwSM7ebb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import gc\n",
        "%matplotlib inline\n",
        "sns.set()\n",
        "\n",
        "from tqdm import tqdm_notebook\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Dropout\n",
        "import keras as K\n",
        "from keras.callbacks import EarlyStopping\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import os\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "n2v = KeyedVectors.load_word2vec_format(\"n2v.bin\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX53zdEX7mRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "edges = pd.read_csv('finnet_data/edges.csv')\n",
        "vertices = pd.read_csv('finnet_data/vertices.csv')\n",
        "ids = pd.read_csv('finnet_data/ids.csv')\n",
        "\n",
        "# Here we convert columns to str and do some region_code preprocessing\n",
        "vertices['main_okved'] = vertices['main_okved'].astype(str)\n",
        "vertices['region_code'] = vertices['region_code'].astype(str)\n",
        "vertices['region_code'][vertices['region_code'] == '82'] = '41'\n",
        "vertices['region_code'][vertices['region_code'] == '81'] = '59'\n",
        "vertices['region_code'][vertices['region_code'] == '80'] = '75'\n",
        "vertices['region_code'][vertices['region_code'].isin(['84', '88'])] = '24'\n",
        "vertices['region_code'][vertices['region_code'] == '85'] = '38'\n",
        "vertices['region_code'][vertices['region_code'] == '0'] = '16'\n",
        "vertices['region_code'][vertices['region_code'] == '99'] = '77'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkdsHKUU8EBv",
        "colab_type": "text"
      },
      "source": [
        "# 1. Data preprocessing\n",
        "### 1.1. Aggregations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aReTawyS7nk1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Simple aggregations by id from edges\n",
        "groupped = pd.concat((edges, edges.rename(columns={'id_1':'id_2', 'id_2':'id_1'}))).drop('id_2', axis=1).groupby('id_1')\n",
        "ids_agg = groupped.agg(['mean', 'sum', 'min', 'max', 'std', 'count'])\n",
        "ids_agg.columns = ['_'.join(el) for el in ids_agg.columns]\n",
        "ids_agg = ids_agg.reset_index().rename(columns={'id_1':'id'}).fillna(0).drop('value_count', axis=1)\n",
        "\n",
        "edges_conc = pd.concat((edges, edges.rename(columns={'id_1':'id_2', 'id_2':'id_1'}))).rename(columns={'id_1':'id'}).merge(vertices[['id', 'main_okved', 'region_code', 'company_type']], on='id').drop('id_2', axis=1)\n",
        "\n",
        "# Simple aggregations by main_okved from edges\n",
        "main_okved_aggs_1 = edges_conc.groupby(['main_okved'])[['value', 'n_transactions']].agg(['mean', 'sum', 'max', 'min', 'count', 'std'])\n",
        "main_okved_aggs_1.columns = ['_'.join(el)+'_okved' for el in main_okved_aggs_1.columns]\n",
        "main_okved_aggs_1 = main_okved_aggs_1.drop('value_count_okved', axis=1).fillna(0)\n",
        "\n",
        "# Simple aggregations by region_code from edges\n",
        "region_aggs_1 = edges_conc.groupby(['region_code'])[['value', 'n_transactions']].agg(['mean', 'sum', 'max', 'min', 'count', 'std'])\n",
        "region_aggs_1.columns = ['_'.join(el)+'_region' for el in region_aggs_1.columns]\n",
        "region_aggs_1 = region_aggs_1.drop('value_count_region', axis=1).fillna(0)\n",
        "\n",
        "# Simple aggregations by company_type from edges\n",
        "company_aggs_1 = edges_conc.groupby(['company_type'])[['value', 'n_transactions']].agg(['mean', 'sum', 'max', 'min', 'count', 'std'])\n",
        "company_aggs_1.columns = ['_'.join(el)+'_company' for el in company_aggs_1.columns]\n",
        "company_aggs_1 = company_aggs_1.drop('value_count_company', axis=1).fillna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKfYgYzT8-4w",
        "colab_type": "text"
      },
      "source": [
        "### 1.2. Node2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Fu6ZyQD7nh6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert n2v vectors to pandas.DataFrame\n",
        "keys = np.array(list(n2v.vocab.keys()))\n",
        "vectors = n2v[n2v.vocab.keys()]\n",
        "n2v_df = pd.DataFrame(vectors, index=keys).add_prefix('vec_').reset_index().rename(columns={'index':'id'})\n",
        "n2v_df['id'] = n2v_df['id'].astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPRiJEUW9TGT",
        "colab_type": "text"
      },
      "source": [
        "### 1.3. Count edges for each main_okved and region_code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOORcKhh7nd-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# OKVED Counter\n",
        "okved2okved = edges[['id_1', 'id_2']].merge(vertices[['id', 'main_okved']].add_suffix('_1'),  on='id_1', how='left').merge(vertices[['id', 'main_okved']].add_suffix('_2'), on='id_2', how='left').drop(['id_1', 'id_2'], axis=1).astype(str)\n",
        "okved2okved = pd.concat((okved2okved, okved2okved.rename(columns={'main_okved_1':'main_okved_2', 'main_okved_2':'main_okved_1'})[['main_okved_1', 'main_okved_2']]))\n",
        "okved2okved.head()\n",
        "\n",
        "okved_list = list(okved2okved['main_okved_1'].unique())\n",
        "okved_df_proba = pd.DataFrame(np.zeros((len(okved_list),len(okved_list))), index=okved_list, columns=okved_list).astype(int)\n",
        "groupped = okved2okved.groupby('main_okved_1')['main_okved_2'].value_counts()\n",
        "for idx in tqdm_notebook(groupped.index):\n",
        "    okved_df_proba.loc[idx[0], idx[1]] += groupped.loc[idx]\n",
        "\n",
        "\n",
        "# Region Counter\n",
        "region2region = edges[['id_1', 'id_2']].merge(vertices[['id', 'region_code']].add_suffix('_1'),  on='id_1', how='left').merge(vertices[['id', 'region_code']].add_suffix('_2'), on='id_2', how='left').drop(['id_1', 'id_2'], axis=1)\n",
        "region2region = pd.concat((region2region, region2region.rename(columns={'region_code_1':'region_code_2', 'region_code_2':'region_code_1'})[['region_code_1', 'region_code_2']]))\n",
        "\n",
        "region_list = list(vertices['region_code'].unique())\n",
        "region_df_proba = pd.DataFrame(np.zeros((len(region_list),len(region_list))), index=list(map(str, region_list)), columns=list(map(str, region_list))).astype(int)\n",
        "groupped = region2region.groupby('region_code_1')['region_code_2'].value_counts()\n",
        "for idx in tqdm_notebook(groupped.index):\n",
        "    region_df_proba.loc[idx[0], idx[1]] += groupped.loc[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ILk2c-w_PZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert counts to probas\n",
        "region_df_proba = (region_df_proba/region_df_proba.sum()).T\n",
        "okved_df_proba = (okved_df_proba/okved_df_proba.sum()).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21RUg43E-U2x",
        "colab_type": "text"
      },
      "source": [
        "### 1.3.1. Dimension reduction for okved by seq2seq\n",
        "Here is simple non-linear autoencoder where middle layer is a representation of our **okved_df_proba** matrix. It reduce size from 1073 to 128 for each **main_okved**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40byabZ67nbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "okved = okved_df_proba\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(okved.shape[0], input_shape=(okved.shape[1],), activation='sigmoid'))\n",
        "model.add(Dense(512, activation='sigmoid'))\n",
        "model.add(Dense(256, activation='sigmoid'))\n",
        "model.add(Dense(128, activation='sigmoid')) # Our representation vector\n",
        "model.add(Dense(256, activation='sigmoid'))\n",
        "model.add(Dense(512, activation='sigmoid'))\n",
        "model.add(Dense(okved.shape[1], activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
        "model.fit(okved.values, okved.values, epochs=20, batch_size=10)\n",
        "\n",
        "new_model = Sequential()\n",
        "for i in range(4):\n",
        "    new_model.add(model.layers[i])\n",
        "new_model.summary()\n",
        "okved = pd.DataFrame(new_model.predict(okved.values), index=okved.index).reset_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yksRNA_R_xsC",
        "colab_type": "text"
      },
      "source": [
        "### 1.4. Region and OKVED stats from  **ยง1.3**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6iwaIcs7nYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "okved_stats = pd.DataFrame({\n",
        "    'sum_okved': okved_df_proba.sum(axis=0),\n",
        "    'mean_okved': okved_df_proba.mean(axis=0),\n",
        "    'std_okved': okved_df_proba.std(axis=0),\n",
        "    'max_okved': okved_df_proba.max(axis=0),\n",
        "}).reset_index().rename(columns={'index':'main_okved'})\n",
        "region_stats = pd.DataFrame({\n",
        "    'sum_region': region_df_proba.sum(axis=0),\n",
        "    'mean_region': region_df_proba.mean(axis=0),\n",
        "    'std_region': region_df_proba.std(axis=0),\n",
        "    'max_region': region_df_proba.max(axis=0),\n",
        "}).reset_index().rename(columns={'index':'region_code'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp7n-ipzBuuZ",
        "colab_type": "text"
      },
      "source": [
        "### 1.5. Merging **ยง1.1** and **ยง1.4**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC1vNzaq7nVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "main_okved_aggs_1 = main_okved_aggs_1.merge(okved_stats, on='main_okved', how='left')\n",
        "region_aggs_1 = region_aggs_1.merge(region_stats, on='region_code', how='left')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnkgtrMyJtT1",
        "colab_type": "text"
      },
      "source": [
        "### 1.6. TF-IDF for okved and main_okved and region_code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkPx93je7nUV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vertices['main_okved_splitted'] = vertices['main_okved'].apply(lambda x: x.split('.')[0])\n",
        "tmp = edges.merge(vertices[['id', 'main_okved_splitted']].add_suffix('_1'), on='id_1').merge(vertices[['id', 'main_okved_splitted']].add_suffix('_2'), on='id_2')[['main_okved_splitted_1', 'main_okved_splitted_2']]\n",
        "corpus = (tmp['main_okved_splitted_1']+' '+tmp['main_okved_splitted_2']).values\n",
        "okved_vec = TfidfVectorizer()\n",
        "okved_vec.fit(corpus)\n",
        "vertices = vertices.drop('main_okved_splitted', axis=1)\n",
        "\n",
        "tmp = edges.merge(vertices[['id', 'region_code']].add_suffix('_1'), on='id_1').merge(vertices[['id', 'region_code']].add_suffix('_2'), on='id_2')[['region_code_1', 'region_code_2']]\n",
        "corpus = (tmp['region_code_1']+' '+tmp['region_code_2']).values\n",
        "region_vec = TfidfVectorizer()\n",
        "region_vec.fit(corpus)\n",
        "del corpus; gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6sfKNwjLCRG",
        "colab_type": "text"
      },
      "source": [
        "# 2. Models training\n",
        "If you do not wanna see big bunch of models on different types of data just go to the **Ensemble** section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD8RlzBqZOWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare directory for models results\n",
        "if not 'Results' in os.listdir():\n",
        "    os.mkdir('Results')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEOR04E57nQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1) TFIDF\n",
        "result = pd.DataFrame(columns=['id_1', 'id_2', 'proba'])\n",
        "for i in tqdm_notebook(ids.id):\n",
        "    df1 = edges[edges['id_1'] == i].reset_index()\n",
        "    df2 = edges[edges['id_2'] == i].reset_index()\n",
        "\n",
        "    df = df1[['id_2', 'id_1']].rename(columns={'id_1':'id_2', 'id_2':'id_1'}).append(df2[['id_1', 'id_2']])\n",
        "    df['target'] = 1\n",
        "    df = vertices.set_index('id').join(df.rename(columns={'id_1':'id'}).set_index('id')['target']).fillna(0)\n",
        "    \n",
        "    corpus_okved = (vertices['main_okved_splitted'][vertices['id'] == i].iloc[0] + ' ' + df['main_okved_splitted'])\n",
        "    corpus_region = (vertices['region_code'][vertices['id'] == i].iloc[0] + ' ' + df['region_code'])\n",
        "    df = pd.concat((df, pd.DataFrame(okved_vec.transform(corpus_okved).toarray(), index=df.index).add_suffix('_okved')), axis=1)\n",
        "    df = pd.concat((df, pd.DataFrame(region_vec.transform(corpus_region).toarray(), index=df.index).add_suffix('_region')), axis=1)\n",
        "\n",
        "    X = df.drop(['target'], axis=1)\n",
        "    y = df['target']\n",
        "    \n",
        "    model = CatBoostClassifier(iterations=100, task_type='GPU', random_state=42, loss_function='Logloss', verbose=0)\n",
        "    model.fit(X, y, [0, 1, 2])\n",
        "    \n",
        "    preds = model.predict_proba(df.drop(['target'], axis=1))[:, 1]\n",
        "    df['preds'] = preds\n",
        "    df['id_2'] = i\n",
        "    \n",
        "    res = df[df['target'] != 1].sort_values(by='preds', ascending=False).reset_index()[['id', 'id_2', 'preds']]\n",
        "    res.columns = ['id_1', 'id_2', 'proba']\n",
        "    \n",
        "    result = result.append(res[:10000], ignore_index=True, sort=False)\n",
        "\n",
        "# Drop duplicates from results\n",
        "id_1 = []\n",
        "id_2 = []\n",
        "for row in result[['id_1', 'id_2']].values:\n",
        "    id_1.append(min(row[0], row[1]))\n",
        "    id_2.append(max(row[0], row[1]))\n",
        "result['id_1'] = id_1\n",
        "result['id_2'] = id_2\n",
        "result = result.drop_duplicates(['id_1', 'id_2'])\n",
        "# Save\n",
        "result.to_csv('Results/tfidf.csv', index=False)\n",
        "\n",
        "# Reset vertices\n",
        "verices = vertices.iloc[:, :4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq-bLKrU7nOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2) Node2Vec vectors + cosine similarity between ids + main_okved_proba + region_proba\n",
        "\n",
        "vertices = vertices.merge(n2v_df, on='id', how='left').fillna(0)\n",
        "\n",
        "result = pd.DataFrame(columns=['id_1', 'id_2', 'proba'])\n",
        "for i in tqdm_notebook(ids.id):\n",
        "    df1 = edges[edges['id_1'] == i].reset_index()\n",
        "    df2 = edges[edges['id_2'] == i].reset_index()\n",
        "\n",
        "    df = df1[['id_2', 'id_1']].rename(columns={'id_1':'id_2', 'id_2':'id_1'}).append(df2[['id_1', 'id_2']])\n",
        "    df['target'] = 1\n",
        "\n",
        "    vecs = n2v.wv.most_similar(str(i), topn=len(keys))\n",
        "    vecs = pd.DataFrame(vecs, columns=['id', 'similarity'])\n",
        "    vecs['id'] = vecs['id'].astype(int)\n",
        "    df = vertices.merge(vecs, on='id').fillna(-1).set_index('id').join(df.rename(columns={'id_1':'id'}).set_index('id')['target']).fillna(0)\n",
        "    \n",
        "    df['okved_proba'] = okved_df_proba.loc[vertices[vertices['id'] == i]['main_okved'], df['main_okved']].values.T.reshape(-1)\n",
        "    df['region_proba'] = region_df_proba.loc[vertices[vertices['id'] == i]['region_code'], df['region_code']].values.T.reshape(-1)\n",
        "\n",
        "    df = vertices.set_index('id').join(df.rename(columns={'id_1':'id'}).set_index('id')['target']).fillna(0)\n",
        "    \n",
        "    X = df.drop(['target'], axis=1)\n",
        "    y = df['target']\n",
        "    \n",
        "    model = CatBoostClassifier(iterations=100, task_type='GPU', random_state=42, loss_function='Logloss', verbose=0)\n",
        "    model.fit(X, y, [0, 1, 2])\n",
        "    \n",
        "    preds = model.predict_proba(df.drop(['target'], axis=1))[:, 1]\n",
        "    df['preds'] = preds\n",
        "    df['id_2'] = i\n",
        "    \n",
        "    res = df[df['target'] != 1].sort_values(by='preds', ascending=False).reset_index()[['id', 'id_2', 'preds']]\n",
        "    res.columns = ['id_1', 'id_2', 'proba']\n",
        "    \n",
        "    result = result.append(res[:10000], ignore_index=True, sort=False)\n",
        "\n",
        "# Drop duplicates from results\n",
        "id_1 = []\n",
        "id_2 = []\n",
        "for row in result[['id_1', 'id_2']].values:\n",
        "    id_1.append(min(row[0], row[1]))\n",
        "    id_2.append(max(row[0], row[1]))\n",
        "result['id_1'] = id_1\n",
        "result['id_2'] = id_2\n",
        "result = result.drop_duplicates(['id_1', 'id_2'])\n",
        "# Save\n",
        "result.to_csv('Results/n2v_df_sim_okved_region_proba_not_seq.csv', index=False)\n",
        "\n",
        "# Reset vertices\n",
        "verices = vertices.iloc[:, :4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRJr69Fy7nLO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3) Node2Vec vectors + cosine similarity between ids + main_okved_aggs + region_aggs\n",
        "\n",
        "vertices = vertices.merge(n2v_df, on='id', how='left').fillna(0)\n",
        "vertices = vertices.merge(main_okved_aggs_1, on='main_okved', how='left').fillna(0)\n",
        "vertices = vertices.merge(region_aggs_1, on='region_code', how='left').fillna(0)\n",
        "\n",
        "result = pd.DataFrame(columns=['id_1', 'id_2', 'proba'])\n",
        "for i in tqdm_notebook(ids.id):\n",
        "    df1 = edges[edges['id_1'] == i].reset_index()\n",
        "    df2 = edges[edges['id_2'] == i].reset_index()\n",
        "\n",
        "    df = df1[['id_2', 'id_1']].rename(columns={'id_1':'id_2', 'id_2':'id_1'}).append(df2[['id_1', 'id_2']])\n",
        "    df['target'] = 1\n",
        "\n",
        "    vecs = n2v.wv.most_similar(str(i), topn=len(keys))\n",
        "    vecs = pd.DataFrame(vecs, columns=['id', 'similarity'])\n",
        "    vecs['id'] = vecs['id'].astype(int)\n",
        "    df = vertices.merge(vecs, on='id').fillna(-1).set_index('id').join(df.rename(columns={'id_1':'id'}).set_index('id')['target']).fillna(0)\n",
        "    df = vertices.set_index('id').join(df.rename(columns={'id_1':'id'}).set_index('id')['target']).fillna(0)\n",
        "    \n",
        "    X = df.drop(['target'], axis=1)\n",
        "    y = df['target']\n",
        "    \n",
        "    model = CatBoostClassifier(iterations=100, task_type='GPU', random_state=42, loss_function='Logloss', verbose=0)\n",
        "    model.fit(X, y, [0, 1, 2])\n",
        "    \n",
        "    preds = model.predict_proba(df.drop(['target'], axis=1))[:, 1]\n",
        "    df['preds'] = preds\n",
        "    df['id_2'] = i\n",
        "    \n",
        "    res = df[df['target'] != 1].sort_values(by='preds', ascending=False).reset_index()[['id', 'id_2', 'preds']]\n",
        "    res.columns = ['id_1', 'id_2', 'proba']\n",
        "    \n",
        "    result = result.append(res[:10000], ignore_index=True, sort=False)\n",
        "\n",
        "# Drop duplicates from results\n",
        "id_1 = []\n",
        "id_2 = []\n",
        "for row in result[['id_1', 'id_2']].values:\n",
        "    id_1.append(min(row[0], row[1]))\n",
        "    id_2.append(max(row[0], row[1]))\n",
        "result['id_1'] = id_1\n",
        "result['id_2'] = id_2\n",
        "result = result.drop_duplicates(['id_1', 'id_2'])\n",
        "# Save\n",
        "result.to_csv('Results/n2v_df_sim_okved_region_aggs.csv', index=False)\n",
        "\n",
        "# Reset vertices\n",
        "verices = vertices.iloc[:, :4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rpi2oKnueTUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 4) Node2Vec vectors + main_okved_aggs + region_aggs\n",
        "\n",
        "vertices = vertices.merge(n2v_df, on='id', how='left').fillna(0)\n",
        "vertices = vertices.merge(main_okved_aggs_1, on='main_okved', how='left').fillna(0)\n",
        "vertices = vertices.merge(region_aggs_1, on='region_code', how='left').fillna(0)\n",
        "\n",
        "result = pd.DataFrame(columns=['id_1', 'id_2', 'proba'])\n",
        "for i in tqdm_notebook(ids.id):\n",
        "    df1 = edges[edges['id_1'] == i].reset_index()\n",
        "    df2 = edges[edges['id_2'] == i].reset_index()\n",
        "\n",
        "    df = df1[['id_2', 'id_1']].rename(columns={'id_1':'id_2', 'id_2':'id_1'}).append(df2[['id_1', 'id_2']])\n",
        "    df['target'] = 1\n",
        "\n",
        "    df = vertices.set_index('id').join(df.rename(columns={'id_1':'id'}).set_index('id')['target']).fillna(0)\n",
        "    df = vertices.set_index('id').join(df.rename(columns={'id_1':'id'}).set_index('id')['target']).fillna(0)\n",
        "    \n",
        "    X = df.drop(['target'], axis=1)\n",
        "    y = df['target']\n",
        "    \n",
        "    model = CatBoostClassifier(iterations=100, task_type='GPU', random_state=42, loss_function='Logloss', verbose=0)\n",
        "    model.fit(X, y, [0, 1, 2])\n",
        "    \n",
        "    preds = model.predict_proba(df.drop(['target'], axis=1))[:, 1]\n",
        "    df['preds'] = preds\n",
        "    df['id_2'] = i\n",
        "    \n",
        "    res = df[df['target'] != 1].sort_values(by='preds', ascending=False).reset_index()[['id', 'id_2', 'preds']]\n",
        "    res.columns = ['id_1', 'id_2', 'proba']\n",
        "    \n",
        "    result = result.append(res[:10000], ignore_index=True, sort=False)\n",
        "\n",
        "# Drop duplicates from results\n",
        "id_1 = []\n",
        "id_2 = []\n",
        "for row in result[['id_1', 'id_2']].values:\n",
        "    id_1.append(min(row[0], row[1]))\n",
        "    id_2.append(max(row[0], row[1]))\n",
        "result['id_1'] = id_1\n",
        "result['id_2'] = id_2\n",
        "result = result.drop_duplicates(['id_1', 'id_2'])\n",
        "# Save\n",
        "result.to_csv('Results/n2v_df_okved_region_aggs.csv', index=False)\n",
        "\n",
        "# Reset vertices\n",
        "verices = vertices.iloc[:, :4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEp7V_l7eTOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 5) main_okved_aggs + region_aggs + company_aggs + okved_proba\n",
        "\n",
        "vertices = vertices.merge(company_aggs_1.reset_index(), on='company_type', how='left').fillna(0)\n",
        "vertices = vertices.merge(main_okved_aggs_1, on='main_okved', how='left').fillna(0)\n",
        "vertices = vertices.merge(region_aggs_1, on='region_code', how='left').fillna(0)\n",
        "vertices = vertices.merge(okved.add_suffix('_4'), left_on='main_okved', right_on='index_4').drop('index_4', axis=1)\n",
        "\n",
        "result = pd.DataFrame(columns=['id_1', 'id_2', 'proba'])\n",
        "for i in tqdm_notebook(ids.id):\n",
        "    df1 = edges[edges['id_1'] == i].reset_index()\n",
        "    df2 = edges[edges['id_2'] == i].reset_index()\n",
        "\n",
        "    df = df1[['id_2', 'id_1']].rename(columns={'id_1':'id_2', 'id_2':'id_1'}).append(df2[['id_1', 'id_2']])\n",
        "    df['target'] = 1\n",
        "\n",
        "    df = vertices.set_index('id').join(df.rename(columns={'id_1':'id'}).set_index('id')['target']).fillna(0)\n",
        "    df = vertices.set_index('id').join(df.rename(columns={'id_1':'id'}).set_index('id')['target']).fillna(0)\n",
        "    \n",
        "    X = df.drop(['target'], axis=1)\n",
        "    y = df['target']\n",
        "    \n",
        "    model = CatBoostClassifier(iterations=100, task_type='GPU', random_state=42, loss_function='Logloss', verbose=0)\n",
        "    model.fit(X, y, [0, 1, 2])\n",
        "    \n",
        "    preds = model.predict_proba(df.drop(['target'], axis=1))[:, 1]\n",
        "    df['preds'] = preds\n",
        "    df['id_2'] = i\n",
        "    \n",
        "    res = df[df['target'] != 1].sort_values(by='preds', ascending=False).reset_index()[['id', 'id_2', 'preds']]\n",
        "    res.columns = ['id_1', 'id_2', 'proba']\n",
        "    \n",
        "    result = result.append(res[:10000], ignore_index=True, sort=False)\n",
        "\n",
        "# Drop duplicates from results\n",
        "id_1 = []\n",
        "id_2 = []\n",
        "for row in result[['id_1', 'id_2']].values:\n",
        "    id_1.append(min(row[0], row[1]))\n",
        "    id_2.append(max(row[0], row[1]))\n",
        "result['id_1'] = id_1\n",
        "result['id_2'] = id_2\n",
        "result = result.drop_duplicates(['id_1', 'id_2'])\n",
        "# Save\n",
        "result.to_csv('Results/okved_region_company_aggs_okved_proba.csv', index=False)\n",
        "\n",
        "# Reset vertices\n",
        "verices = vertices.iloc[:, :4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP9QxuEQeTJF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 6) okved_proba + region_proba + cpu + logloss + 100 iters\n",
        "\n",
        "vertices = vertices.merge(okved.add_suffix('_4'), left_on='main_okved', right_on='index_4').drop('index_4', axis=1)\n",
        "vertices = vertices.merge(region_df_proba.reset_index().add_suffix('_5'), left_on='region_code', right_on='index_5').drop('index_5', axis=1)\n",
        "\n",
        "result = pd.DataFrame(columns=['id_1', 'id_2', 'proba'])\n",
        "for i in tqdm_notebook(ids.id):\n",
        "    df1 = edges[edges['id_1'] == i].reset_index()\n",
        "    df2 = edges[edges['id_2'] == i].reset_index()\n",
        "\n",
        "    df = df1[['id_2', 'id_1']].rename(columns={'id_1':'id_2', 'id_2':'id_1'}).append(df2[['id_1', 'id_2']])\n",
        "    df['target'] = 1\n",
        "\n",
        "    df = vertices.set_index('id').join(df.rename(columns={'id_1':'id'}).set_index('id')['target']).fillna(0)\n",
        "    df = vertices.set_index('id').join(df.rename(columns={'id_1':'id'}).set_index('id')['target']).fillna(0)\n",
        "    \n",
        "    X = df.drop(['target'], axis=1)\n",
        "    y = df['target']\n",
        "    \n",
        "    model = CatBoostClassifier(iterations=100, task_type='CPU', random_state=42, loss_function='Logloss', verbose=0)\n",
        "    model.fit(X, y, [0, 1, 2])\n",
        "    \n",
        "    preds = model.predict_proba(df.drop(['target'], axis=1))[:, 1]\n",
        "    df['preds'] = preds\n",
        "    df['id_2'] = i\n",
        "    \n",
        "    res = df[df['target'] != 1].sort_values(by='preds', ascending=False).reset_index()[['id', 'id_2', 'preds']]\n",
        "    res.columns = ['id_1', 'id_2', 'proba']\n",
        "    \n",
        "    result = result.append(res[:10000], ignore_index=True, sort=False)\n",
        "\n",
        "# Drop duplicates from results\n",
        "id_1 = []\n",
        "id_2 = []\n",
        "for row in result[['id_1', 'id_2']].values:\n",
        "    id_1.append(min(row[0], row[1]))\n",
        "    id_2.append(max(row[0], row[1]))\n",
        "result['id_1'] = id_1\n",
        "result['id_2'] = id_2\n",
        "result = result.drop_duplicates(['id_1', 'id_2'])\n",
        "# Save\n",
        "name = 'okved_region_proba_100i_logloss_cpu.csv'\n",
        "result.to_csv('Results/'+name, index=False)\n",
        "\n",
        "# Reset vertices\n",
        "verices = vertices.iloc[:, :4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXBBBFpCeSzW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 7) okved_proba + region_proba + crossentropy + 300 iters\n",
        "\n",
        "vertices = vertices.merge(okved.add_suffix('_4'), left_on='main_okved', right_on='index_4').drop('index_4', axis=1)\n",
        "vertices = vertices.merge(region_df_proba.reset_index().add_suffix('_5'), left_on='region_code', right_on='index_5').drop('index_5', axis=1)\n",
        "\n",
        "result = pd.DataFrame(columns=['id_1', 'id_2', 'proba'])\n",
        "for i in tqdm_notebook(ids.id):\n",
        "    df1 = edges[edges['id_1'] == i].reset_index()\n",
        "    df2 = edges[edges['id_2'] == i].reset_index()\n",
        "\n",
        "    df = df1[['id_2', 'id_1']].rename(columns={'id_1':'id_2', 'id_2':'id_1'}).append(df2[['id_1', 'id_2']])\n",
        "    df['target'] = 1\n",
        "\n",
        "    df = vertices.set_index('id').join(df.rename(columns={'id_1':'id'}).set_index('id')['target']).fillna(0)\n",
        "    df = vertices.set_index('id').join(df.rename(columns={'id_1':'id'}).set_index('id')['target']).fillna(0)\n",
        "    \n",
        "    X = df.drop(['target'], axis=1)\n",
        "    y = df['target']\n",
        "    \n",
        "    model = CatBoostClassifier(iterations=300, task_type='CPU', random_state=42, loss_function='CrossEntropy', verbose=0)\n",
        "    model.fit(X, y, [0, 1, 2])\n",
        "    \n",
        "    preds = model.predict_proba(df.drop(['target'], axis=1))[:, 1]\n",
        "    df['preds'] = preds\n",
        "    df['id_2'] = i\n",
        "    \n",
        "    res = df[df['target'] != 1].sort_values(by='preds', ascending=False).reset_index()[['id', 'id_2', 'preds']]\n",
        "    res.columns = ['id_1', 'id_2', 'proba']\n",
        "    \n",
        "    result = result.append(res[:10000], ignore_index=True, sort=False)\n",
        "\n",
        "# Drop duplicates from results\n",
        "id_1 = []\n",
        "id_2 = []\n",
        "for row in result[['id_1', 'id_2']].values:\n",
        "    id_1.append(min(row[0], row[1]))\n",
        "    id_2.append(max(row[0], row[1]))\n",
        "result['id_1'] = id_1\n",
        "result['id_2'] = id_2\n",
        "result = result.drop_duplicates(['id_1', 'id_2'])\n",
        "# Save\n",
        "result.to_csv('Results/okved_region_proba_300i_crossentropy.csv', index=False)\n",
        "\n",
        "# Reset vertices\n",
        "verices = vertices.iloc[:, :4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a85uKpPguzyv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 8) main_okved_aggs + region_aggs + company_aggs\n",
        "\n",
        "vertices = vertices.merge(company_aggs_1.reset_index(), on='company_type', how='left').fillna(0)\n",
        "vertices = vertices.merge(main_okved_aggs_1, on='main_okved', how='left').fillna(0)\n",
        "vertices = vertices.merge(region_aggs_1, on='region_code', how='left').fillna(0)\n",
        "\n",
        "result = pd.DataFrame(columns=['id_1', 'id_2', 'proba'])\n",
        "for i in tqdm_notebook(ids.id):\n",
        "    df1 = edges[edges['id_1'] == i].reset_index()\n",
        "    df2 = edges[edges['id_2'] == i].reset_index()\n",
        "\n",
        "    df = df1[['id_2', 'id_1']].rename(columns={'id_1':'id_2', 'id_2':'id_1'}).append(df2[['id_1', 'id_2']])\n",
        "    df['target'] = 1\n",
        "\n",
        "    df = vertices.set_index('id').join(df.rename(columns={'id_1':'id'}).set_index('id')['target']).fillna(0)\n",
        "    df = vertices.set_index('id').join(df.rename(columns={'id_1':'id'}).set_index('id')['target']).fillna(0)\n",
        "    \n",
        "    X = df.drop(['target'], axis=1)\n",
        "    y = df['target']\n",
        "    \n",
        "    model = CatBoostClassifier(iterations=100, task_type='GPU', random_state=42, loss_function='Logloss', verbose=0)\n",
        "    model.fit(X, y, [0, 1, 2])\n",
        "    \n",
        "    preds = model.predict_proba(df.drop(['target'], axis=1))[:, 1]\n",
        "    df['preds'] = preds\n",
        "    df['id_2'] = i\n",
        "    \n",
        "    res = df[df['target'] != 1].sort_values(by='preds', ascending=False).reset_index()[['id', 'id_2', 'preds']]\n",
        "    res.columns = ['id_1', 'id_2', 'proba']\n",
        "    \n",
        "    result = result.append(res[:10000], ignore_index=True, sort=False)\n",
        "\n",
        "# Drop duplicates from results\n",
        "id_1 = []\n",
        "id_2 = []\n",
        "for row in result[['id_1', 'id_2']].values:\n",
        "    id_1.append(min(row[0], row[1]))\n",
        "    id_2.append(max(row[0], row[1]))\n",
        "result['id_1'] = id_1\n",
        "result['id_2'] = id_2\n",
        "result = result.drop_duplicates(['id_1', 'id_2'])\n",
        "# Save\n",
        "result.to_csv('Results/okved_region_company_aggs.csv', index=False)\n",
        "\n",
        "# Reset vertices\n",
        "verices = vertices.iloc[:, :4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhqGr9zVvK16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 9) okved_proba + region_proba + 200 iters\n",
        "vertices = vertices.merge(okved.add_suffix('_4'), left_on='main_okved', right_on='index_4').drop('index_4', axis=1)\n",
        "vertices = vertices.merge(region_df_proba.reset_index().add_suffix('_5'), left_on='region_code', right_on='index_5').drop('index_5', axis=1)\n",
        "\n",
        "result = pd.DataFrame(columns=['id_1', 'id_2', 'proba'])\n",
        "for i in tqdm_notebook(ids.id):\n",
        "    df1 = edges[edges['id_1'] == i].reset_index()\n",
        "    df2 = edges[edges['id_2'] == i].reset_index()\n",
        "\n",
        "    df = df1[['id_2', 'id_1']].rename(columns={'id_1':'id_2', 'id_2':'id_1'}).append(df2[['id_1', 'id_2']])\n",
        "    df['target'] = 1\n",
        "\n",
        "    df = vertices.set_index('id').join(df.rename(columns={'id_1':'id'}).set_index('id')['target']).fillna(0)\n",
        "    df = vertices.set_index('id').join(df.rename(columns={'id_1':'id'}).set_index('id')['target']).fillna(0)\n",
        "    \n",
        "    X = df.drop(['target'], axis=1)\n",
        "    y = df['target']\n",
        "    \n",
        "    model = CatBoostClassifier(iterations=200, task_type='GPU', random_state=42, loss_function='Logloss', verbose=0)\n",
        "    model.fit(X, y, [0, 1, 2])\n",
        "    \n",
        "    preds = model.predict_proba(df.drop(['target'], axis=1))[:, 1]\n",
        "    df['preds'] = preds\n",
        "    df['id_2'] = i\n",
        "    \n",
        "    res = df[df['target'] != 1].sort_values(by='preds', ascending=False).reset_index()[['id', 'id_2', 'preds']]\n",
        "    res.columns = ['id_1', 'id_2', 'proba']\n",
        "    \n",
        "    result = result.append(res[:10000], ignore_index=True, sort=False)\n",
        "\n",
        "# Drop duplicates from results\n",
        "id_1 = []\n",
        "id_2 = []\n",
        "for row in result[['id_1', 'id_2']].values:\n",
        "    id_1.append(min(row[0], row[1]))\n",
        "    id_2.append(max(row[0], row[1]))\n",
        "result['id_1'] = id_1\n",
        "result['id_2'] = id_2\n",
        "result = result.drop_duplicates(['id_1', 'id_2'])\n",
        "# Save\n",
        "result.to_csv('Results/okved_region_proba.csv', index=False)\n",
        "\n",
        "# Reset vertices\n",
        "verices = vertices.iloc[:, :4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZNW00lYRCMp",
        "colab_type": "text"
      },
      "source": [
        "# 3. Ensemble\n",
        "Here we merge all models results and take weighted average as final score "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ddlv8CLY7nIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "merged_df = None\n",
        "for el in other_models:\n",
        "    if merged_df is None:\n",
        "        merged_df = pd.read_csv('Results/'+el)\n",
        "        merged_df['id_1_2'] = merged_df['id_1'].astype(str) + ' ' + merged_df['id_2'].astype(str)\n",
        "        merged_df = merged_df.drop(['id_1', 'id_2'], axis=1)\n",
        "    else:\n",
        "        tmp = pd.read_csv('Results/'+el)\n",
        "        tmp['id_1_2'] = tmp['id_1'].astype(str) + ' ' + tmp['id_2'].astype(str)\n",
        "        tmp = tmp[['id_1_2', 'proba']]\n",
        "        merged_df = merged_df.merge(tmp, on='id_1_2', how='outer')\n",
        "tmp = pd.read_csv('Results/'+name)\n",
        "tmp['id_1_2'] = tmp['id_1'].astype(str) + ' ' + tmp['id_2'].astype(str)\n",
        "tmp = tmp[['id_1_2', 'proba']]\n",
        "merged_df = merged_df.merge(tmp, on='id_1_2', how='outer')\n",
        "merged_df = merged_df.fillna(0)\n",
        "merged_df['mean_proba'] = merged_df.drop('id_1_2', axis=1).mean(axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e63XFiM1I_1",
        "colab_type": "text"
      },
      "source": [
        "So, we have pandas DataFrame with score for different ids pairs. Last step is just to choose top_n value (How many edges we want to predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnnLnGqy10Rf",
        "colab_type": "text"
      },
      "source": [
        "# 4. Final submission\n",
        "In terms of the competition only 100k edges must be chosen. This approach gives ~6400 points on the liderboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tjbRC5V1HeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out = merged_df.sort_values('mean_proba', ascending=False).iloc[:10**5]\n",
        "id_1 = []\n",
        "id_2 = []\n",
        "for el in out['ids'].apply(lambda x: x.split(' ')).values:\n",
        "    id_1.append(el[0])\n",
        "    id_2.append(el[1])\n",
        "out['id_1'] = id_1\n",
        "out['id_2'] = id_2\n",
        "out[['id_1', 'id_2']].to_csv('submission.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}